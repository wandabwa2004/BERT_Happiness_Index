{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sb \n",
    "from TweetHandler import *\n",
    "#from cities_loc import cities_loc\n",
    "from Translator import GoogleTranslate\n",
    "import pandas as pd\n",
    "import time\n",
    "import math \n",
    "#from googletrans import Translator \n",
    "#translator = Translator() \n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import apex\n",
    "\n",
    "from unidecode import unidecode\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#from stop_words import get_stop_words\n",
    "import nltk\n",
    "tknzr = TweetTokenizer()\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')   \n",
    "import re\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_locations  = cities_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Tweets from the specified cities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest():\n",
    "    data = pd.read_csv(\"all_tweets.csv\") # Enter your file location\n",
    "    #data.drop(['username','date','retweets','favorites'], axis=1, inplace=True)\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    data = data[data['Tweet'].isnull() == False]\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', axis=1, inplace=True)\n",
    "    print ('Dataset loaded with shape', data.shape  )  \n",
    "    return data\n",
    "\n",
    "data = ingest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = data.copy() \n",
    "stop += [ 'KOT','kot', '<elong>','com','pic.twitter.com','twitter','rt','que','pic.twitter.com/dnfh8aa3vz','lol']\n",
    "\n",
    "data_copy['Tweet'] = data_copy['Tweet'].map(lambda x:re.sub('[^a-zA-Z]',' ',str(x)))\n",
    "#remove links or anything starting with http\n",
    "data_copy['Tweet'] = data_copy['Tweet'].map(lambda x:re.sub('http.*','',str(x)))\n",
    "#lower case everything \n",
    "data_copy['Tweet'] = data_copy['Tweet'].map(lambda x:re.sub(r'#','',str(x)))\n",
    "data_copy['Tweet'] = data_copy['Tweet'].map(lambda x:re.sub(r'@\\w*','',str(x)))\n",
    "data_copy['Tweet'] = data_copy['Tweet'].map(lambda x:str(x).lower()) \n",
    "data_copy['Tweet'] = data_copy['Tweet'].str.split().map(lambda sl: \" \".join(s for s in sl if len(s) > 3))\n",
    "data_copy['Tweet'] = data_copy['Tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_tweets = data_copy['text']\n",
    "#data_copy.dropna(axis = 1, how ='all', inplace = True) \n",
    "data_copy = data_copy.dropna(axis=0, subset=['Tweet'])\n",
    "count = data_copy['Tweet'].str.split().str.len()\n",
    "data_copy = data_copy[~(count==0)]\n",
    "  #data[~(count==1)]\n",
    "#data_copy = data_copy[~data_copy['Tweet'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator \n",
    "translator = Translator() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data_copy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = data_copy['City']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities.to_csv(\"Cities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = GoogleTranslate(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\") #Insert your Google translate  key\n",
    "    #translator = Translator()\n",
    "translated_tweets = [translator.translate(tweet, \"en\") for tweet in data_copy['Tweet']]\n",
    "\n",
    "translated = pd.DataFrame(translated_tweets)\n",
    "\n",
    "# new_df = pd.concat([pd.DataFrame(translated_tweets),\n",
    "#                        (data3['City'] * len(translated_tweets))],\n",
    "#                        axis=1)\n",
    "# if dataframe.empty:\n",
    "#     dataframe = new_df\n",
    "# else:\n",
    "#        dataframe = pd.concat([dataframe, new_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_csv(\"data1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated.to_csv(\"translated_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2['English'] = data2['Tweet'].apply(translator.translate,dest='en').apply(getattr, args=('text',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cities = pd.read_csv(\"translated_cities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training.csv', encoding='latin-1')\n",
    "\n",
    "# Drop unnecessary columns, leaving behind the [label, text] columns\n",
    "df = df.drop(df.columns[[1, 2, 3, 4]], axis=1)\n",
    "\n",
    "# Rename these columns\n",
    "df.columns = ['label', 'text']\n",
    "\n",
    "# Reverse the format to [text, label]\n",
    "df = df[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_rows = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(num_of_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the rows\n",
    "msk = np.random.rand(len(df)) <= 0.7\n",
    "train = df[msk]\n",
    "valid = df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save back to .csv format\n",
    "train.to_csv('train.csv', index=False)\n",
    "valid.to_csv('valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training  BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "#import apex\n",
    "import os\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from fast_bert.data import BertDataBunch\n",
    "from fast_bert.learner import BertLearner\n",
    "from fast_bert.metrics import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_PATH = './Bert_model/'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenized input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(BERT_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH='./models/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
    "    BERT_MODEL_PATH + 'bert_model.ckpt',\n",
    "BERT_MODEL_PATH + 'bert_config.json',\n",
    "MODEL_PATH + 'pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/'     # path for data files (train and val)\n",
    "LABEL_PATH = './labels/'  # path for labels file\n",
    "MODEL_PATH='./models/'   # path for model artifacts to be stored\n",
    "LOG_PATH='./logs/'      # path for log files to be stored\n",
    "\n",
    "\n",
    "\n",
    "# Default args. If GPU runs out of memory while training, decrease training\n",
    "# batch size\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained(BERT_PRETRAINED_PATH, \n",
    "                                          #do_lower_case=args['do_lower_case'])\n",
    "args = {\n",
    "    \n",
    "    \"max_seq_length\": 512,\n",
    "    \"do_lower_case\": True,\n",
    "    \"train_batch_size\": 8,\n",
    "    \"learning_rate\": 6e-5,\n",
    "    \"num_train_epochs\": 12.0,\n",
    "    \"warmup_proportion\": 0.002,\n",
    "    \"local_rank\": -1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"fp16\": True,\n",
    "    \"loss_scale\": 128\n",
    "}\n",
    "\n",
    "# The tokenizer object is used to split the text into tokens used in training\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model,\n",
    "                                         do_lower_case=args['do_lower_case'])\n",
    "    \n",
    "# Check for GPU                                     \n",
    "device = torch.device('cuda')\n",
    "# check if multiple GPUs are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multi_gpu = True\n",
    "else:\n",
    "    multi_gpu = False\n",
    "\n",
    "# BertDataBunch contains the training, validation, and tests sets, alongside\n",
    "# arguments and the tokenizer used in training\n",
    "databunch = BertDataBunch(DATA_PATH, LABEL_PATH, tokenizer, \n",
    "                          train_file='train.csv', val_file='valid.csv', label_file='labels.csv',\n",
    "                          bs=args['train_batch_size'], maxlen=args['max_seq_length'], \n",
    "                          multi_gpu=multi_gpu, multi_label=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PRETRAINED_PATH = \"./Bert_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the metrics used for the error function in training\n",
    "metrics = []\n",
    "metrics.append({'name': 'accuracy', 'function': accuracy})\n",
    "\n",
    "# The learner contains the logic for training loop, validation loop, \n",
    "# optimiser strategies and key metrics calculation\n",
    "learner = BertLearner.from_pretrained_model(\n",
    "      databunch, BERT_PRETRAINED_PATH, metrics, device, logger, \n",
    "      finetuned_wgts_path=None, is_fp16=args['fp16'], \n",
    "      loss_scale=args['loss_scale'], multi_gpu=multi_gpu, multi_label=False)\n",
    "      \n",
    "# Train the model\n",
    "learner.fit(6, lr=args['learning_rate'], \n",
    "            schedule_type=\"warmup_cosine_hard_restarts\")\n",
    "            \n",
    "# Save the model into a file\n",
    "learner.save_and_reload(MODEL_PATH, \"model_sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_bert.prediction import BertClassificationPredictor\n",
    "\n",
    "predictor = BertClassificationPredictor(model_path=MODEL_PATH, pretrained_path=BERT_PRETRAINED_PATH, \n",
    "                                        label_path=LABEL_PATH, multi_label=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
